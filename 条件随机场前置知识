# 从隐马尔科夫到条件随机场

## 前序知识串联

对于解决分类任务的模型，我们可以将其划分为硬模型和软模型。

- **硬模型：**即为输出即为确值（对于二分类就是0 or 1）

  - **SVM（支持向量机）**：最大支持向量的几何间隔驱动的；
  - **PLA（感知机模型）**：误分类驱动的，$f(w)=sign(w^Tx+b)$。

- **软模型：**即为输出是概率，这里可以再进一步细分为：

  - **概率生成模型**。是对$P(X,Y)$进行建模。

    - **朴素贝叶斯（Naive Bayes）**，朴素贝叶斯这里之所以叫“朴素”，是因为这里做了一个比较强的假设，即$x_i⊥x_j|y（i≠j）$，就是在给定y的情况下，xi与xj是相互独立的，因此可以简写NB假设为：$P(X|y=1/0)=∏P(x_i|y=1/0)$

    - ![image-20220720095918051](https://cdn.jsdelivr.net/gh/junsuyiji/tuchang@master/image-20220720095918051.png)

      如上，用概率图表示NB模型，这里就是也是可以解释道：由于$x_2$与$x_1$间受到Y的阻隔，因此，在给定y的情况下，$x_i$与$x_j$是相互独立的。

    - **隐马尔科夫（Hidden Markov Model）**，当朴素贝叶斯模型中的Y从0/1扩展到Seq(序列)，那么，模型便扩展为了隐马尔科夫模型（HMM）。这里面也有2个假设：

      1. 齐次Markov假设
      2. 观测独立假设

      ![image-20220720100105139](https://cdn.jsdelivr.net/gh/junsuyiji/tuchang@master/image-20220720100105139.png)

  - **概率判别模型。**是对$P(Y|X)$进行建模。

    例如：逻辑回归（Logistics Regression）/SoftMax Regression。这类问题是对于$P(y|x)$进行建模的，利用最大熵思想（Maximum Entropy Model）驱动模型。（PS：在最大熵原则下，如果给定均值和方差，那么Gaussian Dist熵最大）

    - **MEMM（Maximum Entropy Markov Model）最大熵马尔科夫模型**。这是结合了最大熵+HMM的优点的一个模型，属于概率判别模型。概率图模型如下：

      ![image-20220720100131678](https://cdn.jsdelivr.net/gh/junsuyiji/tuchang@master/image-20220720100131678.png)

      PS：这与HMM的模型有点像，但是区别在于：

      1. HMM是生成模型，是对$P(X,Y)$进行建模；MEMM是判别模型，是对$P(Y|X)$进行建模；
      2. 在HMM中，**观测变量是隐变量的输出**；在MEMM中，**观测变量变成输入了**；
      3. HMM中是有**观测独立假设的**；在MEMM中，并不是**观测独立的**（在词性标注任务上，给$x_2$标注的词性$y_2$并不单单与$x_2$关，也与上下文x1、x3有关，因此假设更加合理。

      MEMM的著名问题（缺点）：

      Label Bias Problem（标注偏差问题）：**原因是因为局部归一化。**

    - **CRF（Condition Random Field）条件随机场模型**。为了打破MEMM的标注偏差问题，**将MEMM中的有向变成了无向**，解决了局部归一化问题，变成了**全局归一化**。

      

## HMM VS MEMM

### HMM模型理解

<img src="https://cdn.jsdelivr.net/gh/junsuyiji/tuchang@master/image-20220720110812056.png" alt="image-20220720110812056" style="zoom:50%;" />

HMM可以理解为是泛化的NB模型。其是属于一个概率生成模型。模型参数可以用一个三元组表示：

(2)λ=(π,A,B)

其中：π表示初始状态；A表示状态转移矩阵（就是$Y_t$转移到$Y_t+1$的概率）；B表示发射矩阵（就是Y“射”到X的概率）

HMM具有2个假设，（1）齐次1阶马尔科夫；（2）观测独立假设。下面对着两个强假设进行加以解释说明：

1. **齐次1阶马尔科夫**

   > 1阶怎么理解？

   在马尔科夫链$y_1,y_2,…,y_n$中：

   1阶Markov的通俗理解：$y_3$只与$y_2$有关，$y_2$只与$y_1$有关；

   1阶Markov更专业表述：在给定$y_3$的情况下，$y_4$与$y_2$无关。

   之所以说是1阶，也就是关联链条的长度为1；如果是2阶的话，那就是在给定$y_3$和$y_4$的情况下，$y_5$与$y_2$是无关的。

   1阶Markov的目的是简化计算，因为计算链条变短了。

   > 齐次怎么理解？

   就是在马尔科夫链$y_1,y_2,…,y_n$中，马氏链中的任意$y_t$转移到$y_t+1$所服从的概率分布是相同的，这个就叫做齐次。

   （PS：在链中的任意节点$y_t$，其取值有K多种可能且都是离散的，这里所说的相同的概率分布：是指这K种不同状态之间相互转移的概率，这个是相同不变的）

   用标准化的语言来说：$P(y_t|y_t+1)$这个转移概率与时间t无关。

   齐次1阶Markov用数学表达可以表示成：

   (3)$P(y_t|y_{1:t−1},x_{1:t−1})=P(y_t|y_{t−1})$

2. **观测独立假设**

   用通俗的表述可以理解为：在给定$Y_t$的情况下，$X_t$与其他的$X_k(k≠t)$全都无关。

   用数学表达可以表示成：

   (4)$P(x_t|y_{1:t,x1:t−1})=P(x_t|y_t)$

   观测独立假设的目的还是简化计算，因为不考虑观测序列的内联关系了。

   观测独立假设的来源，其实算得上是从NB上扩展来的（因为HMM算是NB的泛化嘛，[Y从0/1扩展到Seq]）。例如，判断一个邮件是否为垃圾邮件，经典NB模型解决的垃圾邮件分类问题。

   这里我们就做的假设是：每个单词之间是相互独立，当然这个假设是不太合理的，因为一句话中的词语肯定是有着一定语言学上的内在联系的。

### HMM建模

<img src="https://raw.githubusercontent.com/anxiang1836/FigureBed/master/img/HMM-unit.bmp" alt="img" style="zoom:50%;" />

在概率图模型圈出最小单元，如上图所示：（可以很明显看出的是观测独立假设哈），这样根据这个最小单元，我们就很容易可以写出HMM的建模数学表示。

建模对象为：$P(X,Y|λ)$——注意：生成模型建模的是**联合概率分布**

(5)$P(X,Y|λ)=∏_{t=1}^T=P(x_t,y_t|λ)$

$=∏_{t=1}^T = P(y_t|y_{t−1},λ)∙P(x_t|y_t,λ)$



### MEMM模型理解

MEMM打破了HMM的观测独立假设，那么，我们来看MEMM是怎么打破的，还是画出MEMM的最小单元，如下图。

<img src="https://raw.githubusercontent.com/anxiang1836/FigureBed/master/img/MEMM-unit.bmp" alt="img" style="zoom:50%;" />

这图与HMM的最小单元非常相似，唯一的区别在于：对于X与Y之间的发射箭头进行了反转。这时候我们看：给定$Y_t$，这时候$x_{t−1}$与$x_t$是否还是独立了么？

这是一个显然的”V”字型结构，head-to-head，在给定$Y_t$的q情况下，这时候，$x_{t−1}$与$x_t$这个路径就是个连通的了，所以就不再独立了。

于此同时，这样的处理，模型就不再是一个概率生成模型了，就变成了一个概率判别模型了。

> **补充：概率图模型的3种结构形式**
>
> 转自：https://zhuanlan.zhihu.com/p/30139208
>
> D-Separation是一种用来判断变量是否条件独立的图形化方法。换言之，对于一个DAG(有向无环图)E，D-Separation方法可以快速的判断出两个节点之间是否是条件独立的。
>
> - **形式1：head-to-head**
>
> ![image-20220720112718303](https://cdn.jsdelivr.net/gh/junsuyiji/tuchang@master/image-20220720112718303.png)
>
> 在**c未知的条件下，a、b被阻断(blocked)，是独立的**。反过来讲，在c已知的条件下，a、b是连通的，不独立。数学表示是：

> (9)$P(a,b,c)=P(a)∗P(b)∗P(c|a,b)$
>
> (10)$∑cP(a,b,c)=∑cP(a)∗P(b)∗P(c|a,b)$
>
> (11)$P(a,b)=P(a)∗P(b)$
>
> - **形式2：tail-to-tail**
>
> ![image-20220720112830371](https://cdn.jsdelivr.net/gh/junsuyiji/tuchang@master/image-20220720112830371.png)
>
> 在**c给定的条件下，a，b被阻断(blocked)，是独立的**。数学表示是：
> 将式子带入到式子中：(12)$P(a,b,c)=P(c)∗P(a|c)∗P(b|c)$
>
> (13)$P(a,b|c)=P(a,b,c)P(c)$
>
> 将式子1带入到式子2中：
>
> (14)$P(a,b|c)=P(a|c)∗P(b|c)$
>
> - 形式3：head-to-tail
>
> ![image-20220720112919954](https://cdn.jsdelivr.net/gh/junsuyiji/tuchang@master/image-20220720112919954.png)
>
> 在**c给定的条件下，a，b被阻断(blocked)，是独立的**。数学表示是：
> $P(a,b,c)=P(a)P(c|a)P(b|c)$化简后可得：
> ![image-20220720113016310](https://cdn.jsdelivr.net/gh/junsuyiji/tuchang@master/image-20220720113016310.png)

### MEMM建模

![img](https://raw.githubusercontent.com/anxiang1836/FigureBed/master/img/MEMM.jpg)

建模对象：$P(Y|X,λ)$——注意：判别模型建模的是**条件概率分布**

(19)$P(Y|X,λ)=∏_{t=1}^T = P(y_t|y_{t−1},x_{1:T},λ)$

### MEMM特点

- 打破了观测独立假设

> Q:为什么打破观测独立假设要好？
>
> 是因为：对于待标注的序列而言，事实上，序列的内容本不是独立的，也就是说，打破观测独立性会使得模型变得更加合理。

- 从生成式模型转到了判别式模型

> Q:为什么说在这里，判别式的模型比生成式模型要好？
>
> 是因为：任务驱动的，在这样的序列标注任务上，在给定一个待标注序列，我们更关注的是标注的是什么，因此，对于这样的任务我们只需要对条件概率建模就足矣了，而去求联合概率分布就将问题复杂化了（计算也复杂化了）。

### MEMM的缺点

这里，把整个输入X分成了2个部分，。$X=(x_g,x_{1:T})$。即对Y的影响分成两个部分：全局的影响+Local的影响。

<img src="https://raw.githubusercontent.com/anxiang1836/FigureBed/master/img/MEMM_CRF.png" alt="img" style="zoom: 33%;" />

它的主要缺点是会造成Label Bias Problem，（在John Lafferty论文中，指出了MEMM会造成Label Bias Problem问题）。

我们把目标还是放到上文提到的最小单元中，来看一下它里面具有什么问题。

<img src="https://raw.githubusercontent.com/anxiang1836/FigureBed/master/img/MEMM-unit.bmp" alt="img" style="zoom:50%;" />

在这个最小单元我们把它当成一个系统，系统从$y_{t−1}$到$y_t$的这样一个转移用数学可以表示成一个函数，是受$y_{t−1}、y_t$与$x_t$共同作用的一个函数，系统对外是有一定能量的，这个函数被称为Mass Score，而且是大于0的。但是问题出现在：这个Mass Score被局部归一化了。条件概率熵越小，那么对于观测值的考虑越少（有时间看一下John Lafferty的论文）

为了能够更好地理解它的问题所在，这里举一个比较极端的例子。

<img src="https://raw.githubusercontent.com/anxiang1836/FigureBed/master/img/LabelBiasProblem.png" alt="img" style="zoom:50%;" />

比如，我们训练好了这样的一个模型，现有一个观测序列是[r,i,b]。

- 在状态0时，由于P(4|0)>P(1|0)，所以我们选择从0→4转移路径；

- 问题出现在了第二步转移上：由于P(5|4)=1，是受到了局部归一化的影响，从4状态转移仅有1条路径选择，其转移的条件概率为1，其所具有的熵值为0。

  > 为什么从4状态向5转移的条件概率为1？
  >
  > 正是因为MEMM在每一步转移的概率都是要做**局部归一化**的，也就是说，从4状态向外转移的所有的情况加和要为1，在这里，仅有1中转移情况，所以P(5|4)=1

- 在这种情况下，无论观测值为什么，都不会影响4→5的转移，但是，这样标注出来的序列0→4→5→3,[r,o,b]就与客观事实不符了，这也就是问题的所在。

所以，为了解决局部归一化的问题，CRF取消了Y序列的转移方向，也就是取消了局部归一化，改变为了全局归一化。

> 在引入条件随机场（CRF）之前，我们再看一下，有关于随机场的相关概念。
>
> 以下部分概念定义转载自：刘建平老师的https://www.cnblogs.com/pinard/p/7048333.html

## 随机场(RF)

“随机场”的名字取的很玄乎，其实理解起来不难。随机场是由若干个位置组成的整体，当给每一个位置中按照某种分布（或者是某种概率）随机赋予一个值之后，其全体就叫做随机场。

以词性标注为例：

假如我们有10个词形成的句子需要做词性标注。这10个词每个词的词性可以在我们已知的词性集合（名词，动词…)中去选择。当我们为每个词选择完词性后，这就形成了一个随机场。

## 马尔科夫随机场(MRF)

马尔科夫随机场是随机场的特例，它假设随机场中某一个位置的赋值仅仅与和它相邻的位置的赋值有关，和与其不相邻的位置的赋值无关。

换一种表示方式，把马尔科夫随机场映射到无向图中。此无向图中的节点都与某个随机变量相关，连接着节点的边代表与这两个节点有关的随机变量之间的关系。

> 补充1：
>
> 概率无向图模型(probabilistic undirected graphical model)又称为**马尔科夫随机场(Markov Random Field)**，或者马尔科夫网络。也就是说，两个节点之间并没有明确的前后以及方向关系，两个节点之间存在相互作用，与更远出的点没有关系。
>
> 有向图模型通常被称为信念网络(belief network)或者**贝叶斯网络(Bayesian network)**。
>
> 对于这个我们要稍加区分。

继续词性标注为例：（还是10个词的句子）

如果我们假设所有词的词性仅与和它相邻的词的词性有关时，这个随机场就特化成一个马尔科夫随机场。

比如第3个词的词性除了与自己本身的位置有关外，只与第2个词和第4个词的词性有关。

> 补充2：
>
> MRF常用于图像方面——图像分割。
>
> 图像是一个典型的马尔科夫随机场，在图像中每个点可能会和周围的点有关系有牵连，但是和远处的点或者初始点是没有什么关系的，离这个点越近对这个点的影响越大。
>
> 这个很好理解，图像中这个像素点是黑色的，那个很有可能周围也是黑色的像素，但是并不能够推断出距离这个像素点远的像素点们也是黑色的。
>
> 当然这个附近，也就是这个领域的范围和大小，是由我们自己去决定的。

![img](https://raw.githubusercontent.com/anxiang1836/FigureBed/master/img/20191103234823.png)

### MRF因子分解定理

![image-20220720113739423](https://cdn.jsdelivr.net/gh/junsuyiji/tuchang@master/image-20220720113739423.png)

其中，x是个联合概率分布，它的维度是p维；ψ表示势函数；E表示能量函数；K表示最大团的个数；$c_i$表示第i个最大团。

下面对上式子做一个小的变化：
![image-20220720113826167](https://cdn.jsdelivr.net/gh/junsuyiji/tuchang@master/image-20220720113826167.png)

也就是将exp提出来，这样连乘号就变成了连加号了$（e^a × e^b ×e^c = e^{a+b+c}）$。

> 补充：
>
> Q:什么最大团定义？
>
> 通俗理解：就是在无向图中，任意一个子图中，每个节点都能相互的连通，我们就成这个子图为最大团。
>
> 例如，有4个节点的线性链，[a–b–c–d]。在这个线性链中，最大团的个数就是：3个，即为[a–b],[b–c],[c–d]。

## 条件随机场(CRF)

CRF是马尔科夫随机场的特例，它假设马尔科夫随机场中只有𝑋和𝑌两种变量，𝑋一般是给定的，而𝑌一般是在给定𝑋的条件下我们的输出。这样马尔科夫随机场就特化成了条件随机场。

在我们10个词的句子词性标注的例子中，𝑋是词，𝑌是词性。因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个CRF。

对于CRF，我们给出准确的数学语言描述：

设X与Y是随机变量，P(Y|X)是给定X时Y的条件概率分布，若随机变量Y构成的是一个马尔科夫随机场，则称条件概率分布P(Y|X)是条件随机场。

## 线性链条件随机场(Linear-CRF)

注意在CRF的定义中，我们并没有要求𝑋和𝑌有相同的结构。

当𝑋和𝑌有相同结构，即：

$X=(x_1,x_2,…,x_T),Y=(y_1,y_2,…,y_T)$

这个时候，𝑋和𝑌有相同的结构的CRF就构成了线性链条件随机场。

<img src="https://raw.githubusercontent.com/anxiang1836/FigureBed/master/img/CRF_local.png" alt="img" style="zoom:50%;" />



对于Linear-CRF的数学定义是：

设$X=(x_1,x_2,..,x_t,..,x_T),Y=(y_1,y_2,..,y_t,..,y_T)$均为线性链表示的随机变量序列，在给定随机变量序列X的情况下，随机变量Y的条件概率分布P(Y|X)构成条件随机场，即满足马尔科夫性：

$P(y_t|X,y_1,y_2,…,y_T)=P(y_t|X,y_{t−1},y_{t+1})$

则称，$P(Y|X)$为线性链条件随机场。

### Linear-CRF的参数化形式

> 首先，我们先从感性的角度来认识一下。

从《MRF因子分解定理》部分的公式(3)出发：

根据的概率图表示，我们可以将其实际的节点带入进去，

![image-20220720114249038](https://cdn.jsdelivr.net/gh/junsuyiji/tuchang@master/image-20220720114249038.png)

我们单独看线性链中的最大团，根据下图可以很容易看出，对于序列的第t个位置，可以分解上式(2)中的$F(y_{t−1},y_t,x_{1:T})$分解为2个部分，即：$x_{1:T}$对$y_t$的影响以及$y_{t−1}$、$y_t$间的影响。数学化表示为：

![image-20220720114429577](https://cdn.jsdelivr.net/gh/junsuyiji/tuchang@master/image-20220720114429577.png)

其中，$△y_t,x_{1:T}$为状态函数，即表示为在t位置上的节点$y_t$状态；$△y_{t−1},y_t,x_{1:T}$为转移函数，即表示当前节点$y_t$与上一个节点$y_{t−1}$的相关性。

<img src="https://raw.githubusercontent.com/anxiang1836/FigureBed/master/img/CRF_MaxTuan.png" alt="img" style="zoom:50%;" />

> 然后，我们从更数学一点的角度来重新认识Linear-CRF的参数化形式。

![image-20220720115044526](https://cdn.jsdelivr.net/gh/junsuyiji/tuchang@master/image-20220720115044526.png)
