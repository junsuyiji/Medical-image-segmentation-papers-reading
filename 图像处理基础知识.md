# 图像分割相关知识(与GAN相关的知识)



**图像分割是指根据灰度、彩色、空间纹理、几何形状等特征把图像划分成若干个互不相交的区域，使得这些特征在同一区域内表现出一致性或相似性，而在不同区域间表现出明显的不同**

## 基于边缘检测的分割方法

**基于边缘检测的图像分割算法试图通过检测包含不同区域的边缘来解决分割问题**。

**通常不同区域的边界上像素的灰度值变化比较剧烈，如果将图片从空间域通过傅里叶变换到频率域，边缘就对应着高频部分，这是一种非常简单的边缘检测算法。**

边缘检测技术通常可以按照处理的技术分为串行边缘检测和并行边缘检测。串行边缘检测是要想确定当前像素点是否属于检测边缘上的一点，取决于先前像素的验证结果。并行边缘检测是一个像素点是否属于检测边缘的一点取决于当前正在检测的像素点以及与该像素点的一些临近像素点。

最简单的边缘检测方法是并行微分算子法，它利用相邻区域的像素值不连续的性质，采用一阶或者二阶导数来检测边缘点。

近年来还提出了基于曲面拟合的方法、基于边界曲线拟合的方法、基于反应-扩散方程的方法、串行边界查找、基于变形模型的方法。

**边缘检测的优缺点：**
（1）边缘定位准确；
（2）速度快；
（3）不能保证边缘的连续性和封闭性；
（4）在高细节区域存在大量的碎边缘，难以形成一个大区域，但是又不宜将高细节区域分成小碎片；
由于上述的（3）（4）两个难点，边缘检测只能产生边缘点，而非完整意义上的图像分割过程。这也就是说，在边缘点信息获取到之后还需要后续的处理或者其他相关算法相结合才能完成分割任务。
在以后的研究当中，用于提取初始边缘点的自适应阈值选取、用于图像的层次分割的更大区域的选取以及如何确认重要边缘以去除假边缘将变得非常重要。

### 基于边缘分布对齐的对抗网络

关注的主要问题：**无监督域自适应(UDA)问题**，即给定源数据 $X_s$以及对应的像素级标签 $Y_S$，和没有标签目标域数据 $X_T$，方法的目标是学习一个模型 f，**它可以正确地预测目标域数据 $X_T$的像素级标签。**

传统对抗学习的方法有效地提升了模型泛化能力，但这一类方法存在一个先天的 不足：当训练收敛，生成网络能够很好地欺骗判别网络时，它只是对两个领域的边缘分布（Marginal Distribution）进行了对齐（即 P($F_s$) ≈ P ($F_t$), 其中 $F_s$和 $F_t$分别表示源域和目标域中的数据），而忽略了两个领域中相对应的语义类别之间的联合概率分布对齐（即 P($F_s$, $Y_s$) 不等于P ($F_t$, $Y_t$), 其中 $Y_s和 Y_t$表示像素的类别）。因此，传统对抗学习方法在实际训练过程中可能会导致那些已经与源域中的语义对齐很好的目标域像素被映射到一个不正确的语义类别。特别是当使用较大的对抗损失权重训练网络时，这种副作用会变得更为严重，称之为 “负迁移” 现象。

**为了解决减小负迁移现象带来的影响提出了CLAN**

**CLAN 的核心思想分为两个方面。一方面，算法确定那些已经在源域和目标域之间已经语义对齐的像素，并保护这种类别对齐不受对抗学习的副作用影响。另一方面，算法找到在两个域之间分布不同的类，这些类需要利用对抗学习来完成对齐，因此需要在训练中增加了对抗损失的权重。**

## ResNet（基于特征编码）

### 网络变深出现的问题

随着深度学习的应用，各种深度学习模型随之出现，虽然在每年都会出现性能更好的新模型，但是对于前人工作的提升却不是那么明显，其中有重要问题就是深度学习网络在堆叠到一定深度的时候会**出现梯度消失的现象**，导致误差升高效果变差，后向传播时无法将梯度反馈到前面的网络层，使得前方的网络层的参数难以更新，训练效果变差。这个时候ResNet恰好站出来，成为深度学习发展历程中一个重要的转折点。  

 ResNet是由微软研究院的Kaiming He等四名华人提出，他们通过自己提出的ResNet Unit成功训练出来152层的神经网络并在ILSVRC2015比赛中斩获冠军。ResNet语义分割领域最受欢迎且最广泛运用的神经网络.

### 核心思想

**ResNet的核心思想就是在网络中引入恒等映射，允许原始输入信息直接传到后面的层中，在学习过程中可以只学习上一个网络输出的残差（F(x)），因此ResNet又叫做残差网络。**  
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLmxvbGkubmV0LzIwMTkvMDYvMDkvNWNmY2UyMmEwNDhhNDI1ODE4LnBuZw)  
使用到ResNet的分割模型：

* Efficient Neural Network（ENet）：该网络类似于ResNet的bottleNeck方法；

* ResNet-38：该网络在训练or测试阶段增加并移除了一些层，是一种浅层网络，它的结构是ResNet+FCN；

* full-resolution residual network(FRRN)：FRRN网络具有和ResNet相同优越的训练特性，它由残差流和池化流两个处理流组成；

* AdapNey：根据ResNet-50的网络进行改进，让原本的ResNet网络能够在更短的时间内学习到更多高分辨率的特征；  
  ……  

  

  ### ResNet的优缺点：  

  1）引入了全新的网络结构（残差学习模块），形成了新的网络结构，可以使网络尽可能地加深；  
  2）使得前馈/反馈传播算法能够顺利进行，结构更加简单；  
  3）恒等映射地增加基本上不会降低网络的性能；  
  4）建设性地解决了网络训练的越深，误差升高，梯度消失越明显的问题；  
  5）由于ResNet搭建的层数众多，所以需要的训练时间也比平常网络要长。

  ### 网络结构

  ResNet的一个重要设计原则是：**当feature map大小降低一半时，feature map的数量增加一倍，这保持了网络层的复杂度**。从图中可以看到，ResNet相比普通网络每两层间增加了短路机制，这就形成了残差学习，其中虚线表示feature map数量发生了改变。图中展示的34-layer的ResNet，还可以构建更深的网络如表1所示。从表中可以看到，对于18-layer和34-layer的ResNet，其进行的两层间的残差学习，当网络更深时，其进行的是三层间的残差学习，三层卷积核分别是1x1，3x3和1x1，**一个值得注意的是隐含层的feature map数量是比较小的，并且是输出feature map数量的1/4。**

  ![img](https://pic2.zhimg.com/80/v2-7cb9c03871ab1faa7ca23199ac403bd9_1440w.jpg)

  ![img](https://pic1.zhimg.com/80/v2-1dfd4022d4be28392ff44c49d6b4ed94_1440w.jpg)

  

  ### 分析一下残差单元

  **ResNet使用两种残差单元，如图所示。左图对应的是浅层网络，而右图对应的是深层网络。对于短路连接，当输入和输出维度一致时，可以直接将输入加到输出上。但是当维度不一致时（对应的是维度增加一倍），这就不能直接相加。有两种策略：（1）采用zero-padding增加维度，此时一般要先做一个downsamp，可以采用strde=2的pooling，这样不会增加参数；（2）采用新的映射（projection shortcut），一般采用1x1的卷积，这样会增加参数，也会增加计算量。短路连接除了直接使用恒等映射，当然都可以采用projection shortcut。**

  ![img](https://pic1.zhimg.com/80/v2-0892e5423616c30f69ded61111b111c0_1440w.jpg)

  ### ResNet50核心代码的实现

  ```python
  class ResNet50(object):
      def __init__(self, inputs, num_classes=1000, is_training=True,
                   scope="resnet50"):
          self.inputs =inputs
          self.is_training = is_training
          self.num_classes = num_classes
  
          with tf.variable_scope(scope):
              # construct the model
              net = conv2d(inputs, 64, 7, 2, scope="conv1") # -> [batch, 112, 112, 64]
              net = tf.nn.relu(batch_norm(net, is_training=self.is_training, scope="bn1"))
              net = max_pool(net, 3, 2, scope="maxpool1")  # -> [batch, 56, 56, 64]
              net = self._block(net, 256, 3, init_stride=1, is_training=self.is_training,
                                scope="block2")           # -> [batch, 56, 56, 256]
              net = self._block(net, 512, 4, is_training=self.is_training, scope="block3")
                                                          # -> [batch, 28, 28, 512]
              net = self._block(net, 1024, 6, is_training=self.is_training, scope="block4")
                                                          # -> [batch, 14, 14, 1024]
              net = self._block(net, 2048, 3, is_training=self.is_training, scope="block5")
                                                          # -> [batch, 7, 7, 2048]
              net = avg_pool(net, 7, scope="avgpool5")    # -> [batch, 1, 1, 2048]
              net = tf.squeeze(net, [1, 2], name="SpatialSqueeze") # -> [batch, 2048]
              self.logits = fc(net, self.num_classes, "fc6")       # -> [batch, num_classes]
              self.predictions = tf.nn.softmax(self.logits)
  
  
      def _block(self, x, n_out, n, init_stride=2, is_training=True, scope="block"):
          with tf.variable_scope(scope):
              h_out = n_out // 4
              out = self._bottleneck(x, h_out, n_out, stride=init_stride,
                                     is_training=is_training, scope="bottlencek1")
              for i in range(1, n):
                  out = self._bottleneck(out, h_out, n_out, is_training=is_training,
                                         scope=("bottlencek%s" % (i + 1)))
              return out
  
      def _bottleneck(self, x, h_out, n_out, stride=None, is_training=True, scope="bottleneck"):
          """ A residual bottleneck unit"""
          n_in = x.get_shape()[-1]
          if stride is None:
              stride = 1 if n_in == n_out else 2
  
          with tf.variable_scope(scope):
              h = conv2d(x, h_out, 1, stride=stride, scope="conv_1")
              h = batch_norm(h, is_training=is_training, scope="bn_1")
              h = tf.nn.relu(h)
              h = conv2d(h, h_out, 3, stride=1, scope="conv_2")
              h = batch_norm(h, is_training=is_training, scope="bn_2")
              h = tf.nn.relu(h)
              h = conv2d(h, n_out, 1, stride=1, scope="conv_3")
              h = batch_norm(h, is_training=is_training, scope="bn_3")
  
              if n_in != n_out:
                  shortcut = conv2d(x, n_out, 1, stride=stride, scope="conv_4")
                  shortcut = batch_norm(shortcut, is_training=is_training, scope="bn_4")
              else:
                  shortcut = x
              return tf.nn.relu(shortcut + h)
  ```

  [xiaohu2015/DeepLearning_tutorials: The deeplearning algorithms implemented by tensorflow (github.com)](https://github.com/xiaohu2015/DeepLearning_tutorials/)完整的代码

  

## FCN(Fully Convolutional Network)

#### 基于上采样/反卷积的分割方法

**卷积神经网络在进行采样的时候会丢失部分细节信息，这样的目的是得到更具特征的价值。但是这个过程是不可逆的，有的时候会导致后面进行操作的时候图像的分辨率太低，出现细节丢失等问题。因此我们通过上采样在一定程度上可以不全一些丢失的信息，从而得到更加准确的分割边界**

### FCN

CNN能够对图片进行分类，可是怎么样才能识别图片中特定部分的物体，在2015年之前还是一个世界难题。神经网络大神Jonathan Long发表了《Fully Convolutional Networks for Semantic Segmentation》在图像语义分割挖了一个坑，于是无穷无尽的人往坑里面跳。（对，我们也是）

### CNN和FCN

通常CNN网络在卷积层之后会接上若干个全连接层, **将卷积层产生的特征图(feature map)映射成一个固定长度的特征向量。**以AlexNet为代表的经典CNN结构适合于图像级的**分类和回归任务**，因为它们最后都期望得到整个输入图像的一个数值描述（概率），比如AlexNet的ImageNet模型输出一个1000维的向量表示输入图像属于每一类的概率(softmax归一化)。

**FCN对图像进行像素级的分类，从而解决了语义级别的图像分割（semantic segmentation）问题。**与经典的CNN在卷积层之后使用全连接层得到固定长度的特征向量进行分类（全联接层＋softmax输出）不同，**FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。**

最后逐个像素计算softmax分类的损失, 相当于每一个像素对应一个训练样本。下图是Longjon用于语义分割所采用的全卷积网络(FCN)的结构示意图：



![img](https://pic3.zhimg.com/80/v2-92b85d26194124b232a357c04d374856_1440w.jpg)

**简单的来说，FCN与CNN的区别在把于CNN最后的全连接层换成卷积层，输出的是一张已经Label好的图片。**

CNN的强大之处在于它的多层结构能自动学习特征，并且可以学习到多个层次的特征：较浅的卷积层感知域较小，学习到一些局部区域的特征；较深的卷积层具有较大的感知域，能够学习到更加抽象一些的特征。这些抽象特征对物体的大小、位置和方向等敏感性更低，从而有助于识别性能的提高。这些抽象的特征对分类很有帮助，可以很好地判断出一幅图像中包含什么类别的物体，**但是因为丢失了一些物体的细节，不能很好地给出物体的具体轮廓、指出每个像素具体属于哪个物体，因此做到精确的分割就很有难度。**

**传统的基于CNN的分割方法**：为了对一个像素分类，使用该像素周围的一个图像块作为CNN的输入用于训练和预测。这种方法有几个缺点：

1. 是存储开销很大。例如对每个像素使用的图像块的大小为15x15，然后不断滑动窗口，每次滑动的窗口给CNN进行判别分类，因此则所需的存储空间根据滑动窗口的次数和大小急剧上升。
2. 是计算效率低下。相邻的像素块基本上是重复的，针对每个像素块逐个计算卷积，这种计算也有很大程度上的重复。
3. 是像素块大小的限制了感知区域的大小。通常像素块的大小比整幅图像的大小小很多，只能提取一些局部的特征，从而导致分类的性能受到限制。

**而全卷积网络(FCN)则是从抽象的特征中恢复出每个像素所属的类别。即从图像级别的分类进一步延伸到像素级别的分类。**

### **全连接层 替换 成卷积层**

全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：

- **对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块，其余部分都是零。而在其中大部分块中，元素都是相等的。**
- **相反，任何全连接层都可以被转化为卷积层。比如，一个 K=4096 的全连接层，输入数据体的尺寸是 7∗7∗512，这个全连接层可以被等效地看做一个 F=7,P=0,S=1,K=4096 的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致了。因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成 1∗1∗4096，这个结果就和使用初始的那个全连接层一样了。**

**全连接层转化为卷积层**：在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。**假设一个卷积神经网络的输入是 224x224x3 的图像，一系列的卷积层和下采样层将图像数据变为尺寸为 7x7x512 的激活数据体**。AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：

- 针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7，这样输出数据体就为[1x1x4096]了。
- 针对第二个全连接层，令其滤波器尺寸为F=1，这样输出数据体为[1x1x4096]。
- 对最后一个全连接层也做类似的，令其F=1，最终输出为[1x1x1000]

实际操作中，每次这样的变换都需要把**全连接层的权重W重塑成卷积层的滤波器。**那么这样的转化有什么作用呢？它在下面的情况下可以更高效：**让卷积网络在一张更大的输入图片上滑动，得到多个输出，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。**

**举个栗子**：

如果我们想让224×224尺寸的浮窗，以步长为32在384×384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6×6个位置的类别得分。上述的把全连接层转换成卷积层的做法会更简便。如果224×224的输入图片经过卷积层和下采样层之后得到了[7x7x512]的数组，那么，384×384的大图片直接经过同样的卷积层和下采样层之后会得到[12x12x512]的数组。然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出((12 – 7)/1 + 1 = 6)。这个结果正是浮窗在原图经停的6×6个位置的得分！



![img](https://pic2.zhimg.com/80/v2-69b77572b89c4f85a9f58b5a899baae1_1440w.jpg)

FCN将传统CNN中的全连接层转化成卷积层，对应CNN网络FCN把最后三层全连接层转换成为三层卷积层。在传统的CNN结构中，前5层是卷积层，第6层和第7层分别是一个长度为4096的一维向量，第8层是长度为1000的一维向量，分别对应1000个不同类别的概率。**FCN将这3层表示为卷积层，卷积核的大小 (通道数，宽，高) 分别为 (4096,1,1)、(4096,1,1)、(1000,1,1)。看上去数字上并没有什么差别，但是卷积跟全连接是不一样的概念和计算过程，使用的是之前CNN已经训练好的权值和偏置，但是不一样的在于权值和偏置是有自己的范围，属于自己的一个卷积核。因此FCN网络中所有的层都是卷积层，故称为全卷积网络。**

下图是一个全卷积层，与上图不一样的是图像对应的大小下标，CNN中输入的图像大小是同意固定resize成 227x227 大小的图像，第一层pooling后为55x55，第二层pooling后图像大小为27x27，第五层pooling后的图像大小为13*13。而FCN输入的图像是H*W大小，第一层pooling后变为原图大小的1/4，第二层变为原图大小的1/8，第五层变为原图大小的1/16，第八层变为原图大小的1/32（勘误：其实真正代码当中第一层是1/2，以此类推）。

![img](https://pic3.zhimg.com/80/v2-f8de3cac39ab0d8deb551aa0ff815ee2_1440w.jpg)

**过多次卷积和pooling以后，得到的图像越来越小，分辨率越来越低。其中图像到 H/32∗W/32 的时候图片是最小的一层时，所产生图叫做heatmap热图，热图就是我们最重要的高维特征图，得到高维特征的heatmap之后就是最重要的一步也是最后的一步对原图像进行upsampling，把图像进行放大、放大、放大，到原图像的大小。**


![img](https://pic3.zhimg.com/80/v2-b416867a256c9e693e512e596c6d3e16_1440w.jpg)

最后的输出是1000张heatmap经过upsampling变为原图大小的图片，为了对每个像素进行分类预测label成最后已经进行语义分割的图像，这里有一个小trick，就是最后通过逐个像素地求其在1000张图像该像素位置的最大数值描述（概率）作为该像素的分类。因此产生了一张已经分类好的图片，如下图右侧有狗狗和猫猫的图。

![img](https://pic2.zhimg.com/80/v2-74d869decf2019cefde7fa4eb2681b5d_1440w.jpg)

### **upsampling**

相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。这一技巧在实践中经常使用，一次来获得更好的结果。比如，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。

最后，如果我们想用步长小于32的浮窗怎么办？用多次的向前传播就可以解决。比如我们想用步长为16的浮窗。那么先使用原图在转化后的卷积网络执行向前传播，然后分别沿宽度，沿高度，最后同时沿宽度和高度，把原始图片分别平移16个像素，然后把这些平移之后的图分别带入卷积网络。

![img](https://pic3.zhimg.com/v2-e0912566ad8ec451955e2c017e722516_b.webp)



如下图所示，当图片在网络中经过处理后变成越小的图片，其特征也越明显，就像图像中颜色所示，当然啦，最后一层的图片不再是一个1个像素的图片，而是原图像 H/32xW/32 大小的图，这里为了简化而画成一个像素而已。

![img](https://pic4.zhimg.com/80/v2-eb89f44e16c721b88033e73c8c678027_1440w.jpg)

如下图所示，对原图像进行卷积conv1、pool1后原图像缩小为1/2；之后对图像进行第二次conv2、pool2后图像缩小为1/4；接着继续对图像进行第三次卷积操作conv3、pool3缩小为原图像的1/8，此时保留pool3的featureMap；接着继续对图像进行第四次卷积操作conv4、pool4，缩小为原图像的1/16，保留pool4的featureMap；最后对图像进行第五次卷积操作conv5、pool5，缩小为原图像的1/32，然后把原来CNN操作中的全连接变成卷积操作conv6、conv7，图像的featureMap数量改变但是图像大小依然为原图的1/32，此时图像不再叫featureMap而是叫heatMap。

现在我们有1/32尺寸的heatMap，1/16尺寸的featureMap和1/8尺寸的featureMap，1/32尺寸的heatMap进行upsampling操作之后，因为这样的操作还原的图片仅仅是conv5中的卷积核中的特征，限于精度问题不能够很好地还原图像当中的特征，因此在这里向前迭代。把conv4中的卷积核对上一次upsampling之后的图进行反卷积补充细节（相当于一个差值过程），最后把conv3中的卷积核对刚才upsampling之后的图像进行再次反卷积补充细节，最后就完成了整个图像的还原。

![img](https://pic4.zhimg.com/80/v2-c0c7bb87f359bf3d0a553bb7e738267f_1440w.jpg)



### **缺点**

在这里我们要注意的是FCN的缺点：

1. 是得到的结果还是不够精细。进行8倍上采样虽然比32倍的效果好了很多，但是上采样的结果还是比较模糊和平滑，对图像中的细节不敏感。
2. **是对各个像素进行分类，没有充分考虑像素与像素之间的关系。忽略了在通常的基于像素分类的分割方法中使用的空间规整（spatial regularization）步骤，缺乏空间一致性。**这个问题前面的CLAN已经解决了，哈哈哈



参考文章：

[(38条消息) 图像分割综述_计算机视觉life的博客-CSDN博客_图像分割综述](https://blog.csdn.net/electech6/article/details/95242875?ops_request_misc=%7B%22request%5Fid%22%3A%22165061680116781483777922%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=165061680116781483777922&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-95242875.142^v9^control,157^v4^control&utm_term=图像分割综述&spm=1018.2226.3001.4187)

[全卷积网络 FCN 详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/30195134)

[你必须要知道CNN模型：ResNet - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/31852747)





# 图像处理基础知识

1. **位图图像和矢量图形**
2. **图像处理的基本概念**
3. **图像文件的基本格式**
4. **色彩的基本概念**
5. **ps中色彩模式**

## 什么是矢量图形和位图

**矢量图形又称为向量图形，内容以线条和色块为主。制作和处理矢量图形的软件有CorelDRAW、FreeHand、Illustrator和AutoCAD等**

**位图图像又称为点阵图像，它是由许多个点组成的，这些点称为“像素”。制作和处理位图图像的常用软件Photoshop、Corel Photo-Paint、Fireworks等。**

**二者的区别**

1、**分辨率不同**：位图的质量是根据分辨率的大小来判定，分辨率越大，图像的画面质量就更清晰。而矢量图就跟分辨率脱离的关系。在矢量图上没有分辨率这个概念。

2、**图片清晰度不同**：位图放大之后会越来越不清晰，也就是会出现一个个点，就像马赛克一样，就是图片已经出现失真的效果。而矢量图它无限放大都不会出现图像失真的效果，只是它的放大系数参数被改变而已。

3、**针对对象不同**：矢量图适合所针对的是一个对象，也就是一个实体，对每个对象进行编辑都不会影响到其他没有关联的对象。而点位图的编辑受到限制。点位图是点（像素）的排列，局部移动了或者改变了就会影响到其他部分的像素点的排列。

## 图像处理的基本概念

**图像处理是遥感信息提取系统的基本功能模块，主要是面向原始遥感影像数据进行几何校正和增强处理，可以实现图像校正、图像镶嵌、图像裁切、流域裁切、信息增强(数据拉伸、饱和度拉伸、去相关拉伸)、快速配准和影像融合等功能，涵盖了遥感图像前期处理环节必需的各个操作步骤。**各个模块功能的详细说明如下:

**1.图像校正**

在遥感成像过程中，不可避免存在系统性畸变和随机性畸变，通过卫星地面站采购的数据一般经过了几何粗校正处理，本模块主要针对采购回来的遥感影像数据实现几何精校正，即人工选择控制点进行的几何校正，它是用一种数学模型来近似描述遥感图像的几何畸变过程，并利用畸变的遥感图像与标准图像(地图)之间的一些对应点(即控制点数据对)求得这个几何畸变模型，然后利用此模型进行几何畸变的校正，使其具有精准的地理编码。校正对象是没有精确地理坐标的原始遥感影像数据，参考对象是同地区的已经校正过的影像数据，用户交互式地选取地面控制点(GCP)来进行校正，最终输出校正后的影像数据。

**2.图像镶嵌**

在系统实际分析应用中，当研究区处于几幅图像的交界处或研究区较大需多幅图像才能覆盖时，需要把覆盖研究区的那些图像配准，进而把这些图像镶嵌起来，便于更好地统一处理、解译、分析和研究。手工镶嵌存在较大的缺陷，而用计算机进行遥感图像镶嵌则将大大改善镶嵌图像的质量，为后期的图像处理应用创造良好的基础。

该功能完成对相邻两景影像数据的拼接。要求所输入的各景影像数据在地理位置上有重合区域，且为几何精校正后的影像数据，该模块可以自动根据其地理位置进行重叠区的判断，并基于重叠区的统计特征对被调整图像进行色彩调整和边界羽化，从而获得无缝镶嵌、色彩匹配的大幅面镶嵌数据。

**3.图像裁切**

解译分析中需要大范围镶嵌数据，但在数据输出时有更多的实际需求，为此开发了基于标准图幅和任意边界的图像裁切功能。开发了1∶10万和1∶1万标准图幅裁切功能，与中等分辨率监测体系(1∶10万精度)、重点监测区高分辨率监测体系(1∶1万精度)相对应，用户可以自动检测输入数据所包含或所处的图幅，在选择所要裁切的图幅号后就可以很便捷地实现标准分幅数据的裁切。

我国目前河流管理实行流域管理与行政管理并行体系，资源环境数据分别以流域和行政区划为单位进行统计，系统开发了以任意矢量边界裁切输出图像数据的功能，用户调用“四源一干”边界数据或流域范围行政区划边界，可以将万全流域遥感影像数据按照各个流域边界或行政界线进行裁切。

**4.信息增强**

信息增强是遥感图像处理的最基本的方法之一，目的在于:①采用一系列技术改善图像的视觉效果，提高图像的清晰度;②将图像转换成一种更适合于人或机器进行解译和分析处理的形式。该功能不是以图像保真度为原则，而是通过处理设法有选择地突出便于人或机器分析某些感兴趣的信息，抑制一些无用的信息，以提高图像的使用价值，即图像增强处理只是增强了对某些信息的辨别能力。

信息增强是一个相对的概念，增强效果的好坏，除与算法本身的优劣有一定的关系外，还与图像的数据特征有直接关系，同时由于评价图像质量的优劣往往凭观测者的主观而定，没有通用的定量标准，因此增强技术大多属于面向问题，具体增强方法由用户选择地使用。

系统开发了数据拉伸、饱和度拉伸、去相关拉伸等功能，由用户根据需要设置拉伸的范围和数据输出范围。

**5.快速配准**

完成对前后两期数据的快速配准，通过对前后两期数据快速自动选择匹配点，而后直接校正后期数据，从而达到快速配准的目的。

**6.影像融合**

在遥感监测体系构建中，采用MODIS、ETM、ASTER、SPOT等多种不同的传感器获取的遥感影像数据。这些数据在空间、时间、光谱等方面对于同一区域构成多源数据。单一传感器的影像数据通常不能提取足够的信息来完成某些应用。而对多传感器的数据进行融合，可以充分发挥各种传感器影像自身的特点，从而得到更多的信息。这也是开发影像融合功能的目的所在。

系统基于HSV融合法开发了影像融合功能模块，集成了低分辨率数据的重采样，HSV正变换和HSV反变换三个步骤，使得融合操作简单便捷。通过对不同分辨率影像数据融和处理，获得的高分辨率彩色图像既具有较高空间分辨率，同时又具有与影像相同的色调和饱和度，有利于目视解译和信息自动提取。
